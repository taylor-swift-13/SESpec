#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾ªç¯ä¸å˜é‡Testè„šæœ¬ - æ”¯æŒå•DatasetTestå’Œä¸åŒModelæ—¥å¿—åˆ†ç±»
"""

import os
import json
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from main import MainConfig, FunctionProcessor

def list_files_os(folder_path):
    file_names = []
    for entry in os.listdir(folder_path):
        full_path = os.path.join(folder_path, entry)
        if os.path.isfile(full_path):
            file_names.append(entry)
    return file_names

def get_dataset_config(dataset_name, model_name="default"):
    """è·å–ä¸åŒDatasetçš„é…ç½®"""
    configs = {
        'frama-c-loop': {
            'input_dir': './input/frama-c-loop',
            'log_dir': f'./log/frama-c-loop/{model_name}',
            'root_dir': 'frama-c-loop',
            'preconditions': {
            },
            'config_params': {
                'auto_annotation': True,
                'debug': True,
                'only_loop': False,
                'generlization': False,
                'use_db': False,
                'pass_count': 5
            }
        },
         'frama-c-loop-mask': {
            'input_dir': './input/frama-c-loop-mask',
            'log_dir': f'./log/frama-c-loop-mask/{model_name}',
            'root_dir': 'frama-c-loop-mask',
            'preconditions': {
            },
            'config_params': {
                'auto_annotation': True,
                'debug': True,
                'only_loop': False,
                'generlization': False,
                'use_db': True,
                'pass_count': 1
            }
        },
         'pIp': {
            'input_dir': './input/pIp',
            'log_dir': f'./log/pIp/{model_name}',
            'root_dir': 'pIp',
            'preconditions': {
            },
            'config_params': {
                'auto_annotation': True,
                'debug': True,
                'only_loop': False,
                'generlization': False,
                'use_db': True,
                'auto_post': False,
                'pass_count': 5
            }
        },
        'function': {
            'input_dir': './input/function',
            'log_dir': f'./log/function/{model_name}',
            'root_dir': 'function',
            'preconditions': {
            },
            'config_params': {
                'auto_annotation': True,
                'debug': True,
                'template': True,
                'only_loop': False,
                'generlization': False,
                'use_db': True,
                'refine_count': 3,
                'pass_count': 1,
                'auto_post': True
            }
        },
        'LIG-MM': {
            'input_dir': './input/LIG-MM',
            'log_dir': f'./log/LIG-MM/{model_name}',
            'root_dir': 'LIG-MM',
            'preconditions': {
            },
            'config_params': {
                'auto_annotation': True,
                'debug': True,
                'only_loop': True,
                'list_loop': True,
                'generlization': False,
                'use_db': True,
                'auto_post': True,
                'pass_count': 5
            }
        },
        'frama-c-hard': {
            'input_dir': './input/frama-c-hard',
            'log_dir': f'./log/frama-c-hard/{model_name}',
            'root_dir': 'frama-c-hard',
            'preconditions': {
            },
            'config_params': {
                'auto_annotation': True,
                'debug': True,
                'only_loop': False,
                'generlization': False,
                'use_db': True,
                'auto_post': True
            }
        }
    }
    
    
    return configs.get(dataset_name, configs['frama-c-loop'])

def process_single_file(name, dataset_name, config, model_name, log_dir, counters, lock):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹"""
    # é…ç½®å‚æ•°
    original_name = name
    name = name.split('.')[0]
    
    # æ ¹æ®DatasetTypeé€‰æ‹©æ—¥å¿—æ–‡ä»¶å‘½åæ ¼å¼å’Œå‡½æ•°å
    if dataset_name == 'frama-c-loop' or dataset_name == 'frama-c-loop-mask' or dataset_name == 'function':
        log_path = os.path.join(log_dir, f'goo{name}.log')
        function_name = f'goo{name}'
    elif dataset_name == 'pIp':
        name = name.split('_')[1]
        log_path = os.path.join(log_dir, f'main{name}.log')
        function_name = f'main{name}'
    else:
        log_path = os.path.join(log_dir, f'main{name}.log')
        function_name = f'main{name}'
    
    if os.path.exists(log_path):
        print(f"â­ï¸ Skip {name}ï¼Œæ—¥å¿—æ–‡ä»¶already exists")
        return None

    print(f"ğŸ”„ Processing file: {name}")

    # åˆ›å»ºé…ç½®å¯¹è±¡
    config_obj = MainConfig(
        root_dir=config['root_dir'],
        function_name=function_name,
        **config['config_params']
    )

    # è·å–é¢„æ¡ä»¶å­—ç¬¦ä¸²ï¼Œå¹¶å°è£…ä¸ºå­—å…¸ä¼ å…¥å¤„ç†å™¨
    condition_str = config['preconditions'].get(function_name, 'emp')
    preconditions = {function_name: condition_str}

    try:
        # æ‰§è¡Œåˆ†æ
        processor = FunctionProcessor(config_obj, preconditions, model_name)
        processor.run_analysis()
        first_pass = processor.first_pass

        if first_pass is None:
            print(f"âš ï¸ {name}: åˆ†æè¿”å› Noneï¼ŒSkipStatistics")
            return None
        
        if 'syntax' not in first_pass or 'valid' not in first_pass or 'satisfy' not in first_pass:
            print(f"âš ï¸ {name}: åˆ†æResultç¼ºå°‘å¿…è¦çš„é”®ï¼ŒSkipStatistics")
            return None
        
        # StatisticsResult
        syntax_score = first_pass['syntax']
        valid_score = first_pass['valid']
        satisfy_score = first_pass['satisfy']
        
        # ä½¿ç”¨é”æ›´æ–°è®¡æ•°å™¨
        with lock:
            if syntax_score == 1:
                counters['syntax_count'][1] += 1
                counters['syntax_count'][3] += 1
                counters['syntax_count'][5] += 1
            elif syntax_score <= 3:
                counters['syntax_count'][3] += 1
                counters['syntax_count'][5] += 1
            elif syntax_score <= 5:
                counters['syntax_count'][5] += 1
                
            if valid_score == 1:
                counters['valid_count'][1] += 1
                counters['valid_count'][3] += 1
                counters['valid_count'][5] += 1
            elif valid_score <= 3:
                counters['valid_count'][3] += 1
                counters['valid_count'][5] += 1
            elif valid_score <= 5:
                counters['valid_count'][5] += 1
            
            if satisfy_score == 1:
                counters['satisfy_count'][1] += 1
                counters['satisfy_count'][3] += 1
                counters['satisfy_count'][5] += 1
            elif satisfy_score <= 3:
                counters['satisfy_count'][3] += 1
                counters['satisfy_count'][5] += 1
            elif satisfy_score <= 5:
                counters['satisfy_count'][5] += 1
        
        print(f"âœ… {name}: syntax={syntax_score}, valid={valid_score}, satisfy={satisfy_score}")
        return {'syntax': syntax_score, 'valid': valid_score, 'satisfy': satisfy_score}
        
    except Exception as e:
        print(f"âŒ {name}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸError: {str(e)}")
        return None

def test_dataset(dataset_name, model_name="gpt-4o", num_workers=4):
    """TestæŒ‡å®šDatasetï¼ˆæ”¯æŒå¤šçº¿ç¨‹ï¼‰"""
    config = get_dataset_config(dataset_name, model_name)
    
    name_list = list_files_os(config['input_dir'])
    log_dir = config['log_dir']
    
    # åˆå§‹åŒ–è®¡æ•°å™¨ï¼ˆä½¿ç”¨å­—å…¸ä»¥ä¾¿åœ¨çº¿ç¨‹é—´å…±äº«ï¼‰
    counters = {
        'syntax_count': {1: 0, 3: 0, 5: 0},
        'valid_count': {1: 0, 3: 0, 5: 0},
        'satisfy_count': {1: 0, 3: 0, 5: 0}
    }
    lock = threading.Lock()  # ç”¨äºä¿æŠ¤è®¡æ•°å™¨çš„çº¿ç¨‹å®‰å…¨
    
    count = len(name_list)
    print(f"ğŸ” Dataset: {dataset_name}")
    print(f"ğŸ¤– Model: {model_name}")
    print(f"ğŸ“ Input directory: {config['input_dir']}")
    print(f"ğŸ“Š Found {count} fileséœ€è¦å¤„ç†")
    print(f"ğŸ“ Log directory: {log_dir}")
    print(f"ğŸ§µ Using {num_workers} worker threads")
    
    # ç¡®ä¿Log directoryå­˜åœ¨
    os.makedirs(log_dir, exist_ok=True)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(process_single_file, name, dataset_name, config, model_name, log_dir, counters, lock): name
            for name in name_list
        }
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        completed = 0
        for future in as_completed(futures):
            completed += 1
            if completed % 10 == 0:
                print(f"ğŸ“ˆ Progress: {completed}/{count} files processed")
    
    # æå–è®¡æ•°å™¨ç»“æœ
    syntax_count = counters['syntax_count']
    valid_count = counters['valid_count']
    satisfy_count = counters['satisfy_count']

    # OutputStatisticsResult
    print("\n" + "="*60)
    print(f"ğŸ“Š {dataset_name} DatasetTestResultStatistics (æ¨¡å‹: {model_name})")
    print("="*60)
    print(f"Total files: {count}")
    print(f"syntax_count: {syntax_count}")
    print(f"valid_count: {valid_count}")
    print(f"satisfy_count: {satisfy_count}")
    
    if count > 0:
        print(f"\nğŸ“ˆ Pass@1 Statistics:")
        print(f"  syntax_rate: {syntax_count[1]/count:.2%}")
        print(f"  valid_rate: {valid_count[1]/count:.2%}")
        print(f"  satisfy_rate: {satisfy_count[1]/count:.2%}")
        
        print(f"\nğŸ“ˆ Pass@3 Statistics:")
        print(f"  syntax_rate: {syntax_count[3]/count:.2%}")
        print(f"  valid_rate: {valid_count[3]/count:.2%}")
        print(f"  satisfy_rate: {satisfy_count[3]/count:.2%}")
        
        print(f"\nğŸ“ˆ Pass@5 Statistics:")
        print(f"  syntax_rate: {syntax_count[5]/count:.2%}")
        print(f"  valid_rate: {valid_count[5]/count:.2%}")
        print(f"  satisfy_rate: {satisfy_count[5]/count:.2%}")
    
    return {
        'dataset': dataset_name,
        'model': model_name,
        'total_files': count,
        'syntax_count': syntax_count,
        'valid_count': valid_count,
        'satisfy_count': satisfy_count
    }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å‡½æ•°è§„çº¦Testè„šæœ¬ï¼ˆæ”¯æŒå¤šçº¿ç¨‹ï¼‰')
    parser.add_argument('--dataset', type=str, default='pIp', 
                       help='Datasetåç§°ï¼ˆé»˜è®¤: pIpï¼‰')
    parser.add_argument('--model', type=str, default='gpt-4o',
                       help='æ¨¡å‹åç§°ï¼ˆé»˜è®¤: gpt-4oï¼‰')
    parser.add_argument('--workers', type=int, default=4,
                       help='çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸš€ å‡½æ•°è§„çº¦Testè„šæœ¬å¯åŠ¨")
    print("="*60)
    
    DATASET_NAME = args.dataset
    MODEL_NAME = args.model
    NUM_WORKERS = args.workers
    
    print(f"ğŸ“‹ Testé…ç½®:")
    print(f"  Dataset: {DATASET_NAME}")
    print(f"  Model: {MODEL_NAME}")
    print(f"  Workers: {NUM_WORKERS}")
    print(f"  Log directory: ./log/{DATASET_NAME}/{MODEL_NAME}")
    
    # TestæŒ‡å®šDataset
    test_dataset(DATASET_NAME, MODEL_NAME, NUM_WORKERS)
    
    print("\nğŸ‰ Testå®Œæˆï¼")
    print(f"ğŸ“Š Results saved to: ./log/{DATASET_NAME}/{MODEL_NAME}/")

if __name__ == '__main__':
    main() 